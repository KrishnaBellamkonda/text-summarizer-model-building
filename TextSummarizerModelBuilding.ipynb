{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarizer Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the embeddings \n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Loading the tokenization class for bert\n",
    "# The tokenization file is present in the directory of the ipynb\n",
    "import tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer & BERT loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the FullTokenizer\n",
    "# Establishing the tokenizer attributes\n",
    "FullTokenizer = tokenization.FullTokenizer\n",
    "# URL for the 12-layered BERT embeddings \n",
    "# this is uncased - meaning all the words are converted to lower case \n",
    "BERT_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "\n",
    "# Creating the BERT keras layer \n",
    "bert_layer = hub.KerasLayer(BERT_URL, trainable = True)\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "tokenizer = FullTokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "# URL for the 12-layered BERT embeddings \n",
    "# this is uncased - meaning all the words are converted to lower case \n",
    "BERT_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "\n",
    "# Creating the BERT keras layer \n",
    "bert_layer = hub.KerasLayer(BERT_URL, trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40 words Extractive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs \n",
    "max_len = 40 # Need this as the argument \n",
    "input_ids = keras.layers.Input(shape = (max_len, ), \n",
    "                              dtype = tf.int32, name = \"input_ids\")\n",
    "input_mask = keras.layers.Input(shape = (max_len, ), \n",
    "                              dtype = tf.int32, name = \"input_mask\")\n",
    "input_segments= keras.layers.Input(shape = (max_len, ), \n",
    "                              dtype = tf.int32, name = \"input_segment\")\n",
    "\n",
    "\n",
    "# Bert layer \n",
    "# Works with teh BERT_URL\n",
    "BERT_PATH = \"./bert_uncased/bert_cased_L-12_H-768_A-12_1.tar/bert_cased_L-12_H-768_A-12_1\"\n",
    "bert_layer = hub.KerasLayer(BERT_URL, trainable=True)\n",
    "# Bert Outputs \n",
    "sentence_pooled_outputs, sentence_embeddings = bert_layer([input_ids, input_segments, input_mask])\n",
    "\n",
    "# End Lambda Layer\n",
    "lambda_layer = keras.layers.Lambda(lambda inputs: tf.reduce_mean(inputs, axis = 1))\n",
    "out = lambda_layer(sentence_pooled_outputs)\n",
    "\n",
    "# Model Building \n",
    "model = keras.models.Model(inputs = [input_ids, input_segments, input_mask], \n",
    "                          outputs = [out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segment (InputLayer)      [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_2 (KerasLayer)      [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
      "                                                                 input_segment[0][0]              \n",
      "                                                                 input_mask[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None,)              0           keras_layer_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 109,482,241\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model \n",
    "save_path = \"./models/\"\n",
    "model_name = \"ExtractiveSummarizer_v1_40_words\"\n",
    "model.save(save_path, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n",
    "vocab_file = \"./vocab.txt\"\n",
    "with open(vocab_file, \"r\", encoding=\"utf-8\") as fp:\n",
    "    vocab_words = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing vocab files into metadata.tsv \n",
    "metadata_file = \"./metadata.tsv\"\n",
    "with open(metadata_file, \"w\", encoding=\"utf-8\") as fp:\n",
    "    metadata_string = \"\".join(vocab_words)\n",
    "    fp.write(metadata_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights saving \n",
    "weights = tf.Variable(model.layers[3].get_weights()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/embedding.ckpt-1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoints\n",
    "log_dir = \"./logs/\"\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings \n",
    "from tensorboard.plugins import projector\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
